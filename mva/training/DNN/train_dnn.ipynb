{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT as r\n",
    "r.EnableImplicitMT()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_schedulers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transforms={}):\n",
    "        self.transforms = transforms\n",
    "\n",
    "        inputs = []\n",
    "        for var in list(data.keys())[:-3]:\n",
    "            inputs.append(self.transform(torch.tensor(data[var].to_numpy(), dtype=torch.float), self.transforms.get(var, None)))\n",
    "        \n",
    "        self.features = torch.stack(inputs).transpose(0,1)\n",
    "        self.weights = torch.tensor(data.weight.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "        self.labels = torch.tensor(data.label.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "        self.disco_target = torch.tensor(data.VBSBDTscore.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        self.weight_norm = torch.ones(self.labels.size())\n",
    "        self.labels_norm = torch.ones(self.labels.size())\n",
    "\n",
    "        self.weight_norm[self.labels == 0] = torch.sum(self.labels == 0) / torch.sum(self.weights[self.labels == 0])\n",
    "        self.weight_norm[self.labels == 1] = torch.sum(self.labels == 1) / torch.sum(self.weights[self.labels == 1])\n",
    "        self.labels_norm[self.labels == 1] = torch.sum(self.labels == 0) / torch.sum(self.labels == 1)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(feature, transf):\n",
    "        if type(transf) == list:\n",
    "            transf, params = transf\n",
    "        if transf == None:\n",
    "            return feature\n",
    "        elif transf == \"rescale\":\n",
    "            min_value, max_value = params\n",
    "            return (feature - min_value)/(max_value - min_value)\n",
    "        elif transf == \"log\":\n",
    "            if (feature < 0).any():\n",
    "                raise ValueError(\"Log transformation of negative values not supported\")\n",
    "            return torch.log(feature)\n",
    "        elif transf == \"log2\":\n",
    "            return torch.log2(feature)\n",
    "        elif transf == \"log10\":\n",
    "            return torch.log10(feature)\n",
    "        else:\n",
    "            raise ValueError(f\"transformation '{transf}' not supported\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx], self.weights[idx]*self.labels_norm[idx]*self.weight_norm[idx], self.disco_target[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleDisCoLoss(nn.Module):\n",
    "    def __init__(self, disco_lambda, dCorr_power=1):\n",
    "        super().__init__()\n",
    "        self.disco_lambda = disco_lambda\n",
    "        self.dCorr_power = dCorr_power\n",
    "\n",
    "    @staticmethod\n",
    "    def dCorr(var_1, var_2, normed_weight, power):\n",
    "        xx = var_1.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\n",
    "        yy = var_1.repeat(len(var_1),1).view(len(var_1),len(var_1))\n",
    "        amat = (xx-yy).abs()\n",
    "\n",
    "        xx = var_2.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\n",
    "        yy = var_2.repeat(len(var_2),1).view(len(var_2),len(var_2))\n",
    "        bmat = (xx-yy).abs()\n",
    "\n",
    "        amatavg = torch.mean(amat*normed_weight,dim=1)\n",
    "        Amat=amat-amatavg.repeat(len(var_1),1).view(len(var_1),len(var_1))\\\n",
    "            -amatavg.view(-1, 1).repeat(1, len(var_1)).view(len(var_1),len(var_1))\\\n",
    "            +torch.mean(amatavg*normed_weight)\n",
    "\n",
    "        bmatavg = torch.mean(bmat*normed_weight,dim=1)\n",
    "        Bmat=bmat-bmatavg.repeat(len(var_2),1).view(len(var_2),len(var_2))\\\n",
    "            -bmatavg.view(-1, 1).repeat(1, len(var_2)).view(len(var_2),len(var_2))\\\n",
    "            +torch.mean(bmatavg*normed_weight)\n",
    "\n",
    "        ABavg = torch.mean(Amat*Bmat*normed_weight,dim=1)\n",
    "        AAavg = torch.mean(Amat*Amat*normed_weight,dim=1)\n",
    "        BBavg = torch.mean(Bmat*Bmat*normed_weight,dim=1)\n",
    "\n",
    "        if power == 1:\n",
    "            dCorr=(torch.mean(ABavg*normed_weight))/torch.sqrt((torch.mean(AAavg*normed_weight)*torch.mean(BBavg*normed_weight)))\n",
    "        elif power == 2:\n",
    "            dCorr=(torch.mean(ABavg*normed_weight))**2/(torch.mean(AAavg*normed_weight)*torch.mean(BBavg*normed_weight))\n",
    "        else:\n",
    "            dCorr=((torch.mean(ABavg*normed_weight))/torch.sqrt((torch.mean(AAavg*normed_weight)*torch.mean(BBavg*normed_weight))))**power\n",
    "        return dCorr\n",
    "\n",
    "    def forward(self, inferences, labels, disco_target, weights):\n",
    "        BCE = F.binary_cross_entropy(inferences, labels, reduction=\"mean\", weight=weights)\n",
    "        dCorr = self.dCorr(inferences[labels == 0], disco_target[labels == 0], weights[labels == 0], power=self.dCorr_power)\n",
    "        return BCE + self.disco_lambda*dCorr, BCE, self.disco_lambda*dCorr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden_layers, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_layers = []\n",
    "        for layer_i in range(n_hidden_layers):\n",
    "            if layer_i == 0:\n",
    "                hidden_layers.append(nn.Linear(input_size, hidden_size))\n",
    "                hidden_layers.append(nn.ReLU())\n",
    "            else:\n",
    "                hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(signal_filename, background_filename):\n",
    "    def pddf(filename, variables):\n",
    "        data = r.RDataFrame(\"Events\", filename).AsNumpy(variables)\n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    variables = [\"Hbbmass\", \"HbbPt\", \"Wjetmass\", \"WjetPt\",  \"leptonpt\", \"Mlbminloose\", \"MET\", \"VBSBDTscore\", \"weight\"]\n",
    "\n",
    "    x_sig = pddf(signal_filename, variables)\n",
    "    x_bkg = pddf(background_filename, variables)\n",
    "\n",
    "    x_sig[\"label\"] = 1\n",
    "    x_bkg[\"label\"] = 0\n",
    "\n",
    "    df = pd.concat([x_sig, x_bkg], ignore_index=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"/ceph/cms/store/user/aaarora/output/sig_MVA.root\", \"/ceph/cms/store/user/aaarora/output/bkg_MVA.root\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    \"Hbbmass\" : [\"rescale\",[50,250]],\n",
    "    \"HbbPt\"   : \"log\",\n",
    "    \"Wjetmass\" : [\"rescale\",[0,200]],\n",
    "    \"WjetPt\" : \"log\",\n",
    "    \"leptonpt\" : \"log\",\n",
    "    \"Mlbminloose\" : [\"rescale\",[0,1000]],\n",
    "    \"MET\" : \"log\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(df_train, transforms=transform_dict)\n",
    "test_data = MyDataset(df_test, transforms=transform_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches_train, n_batches_test = 10, 5\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=len(train_data)//n_batches_train, shuffle=True, drop_last=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data)//n_batches_test, shuffle=True, drop_last=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(7, 3, 64)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_schedulers.ConstantLR(optimizer, factor=1)\n",
    "criterion = SingleDisCoLoss(30, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = [], []\n",
    "for epoch in tqdm(range(1000), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    tr_loss, tr_bce, tr_disco = 0, 0, 0\n",
    "    te_loss, te_bce, te_disco = 0, 0, 0 \n",
    "    for batch_i, (features, labels, weights, disco_target) in enumerate(train_dataloader):\n",
    "        # train\n",
    "        features, labels, weights, disco_target = features.to(device), labels.to(device), weights.to(device), disco_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        if torch.isnan(predictions).any():\n",
    "            raise ValueError(\"NaN in predictions\")\n",
    "        loss, bce, disco = criterion(predictions, labels, disco_target, weights)\n",
    "        tr_loss += loss.item()\n",
    "        tr_bce += bce.item()\n",
    "        tr_disco += disco.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # test\n",
    "        if batch_i % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for features, labels, weights, disco_target in test_dataloader:\n",
    "                    features, labels, weights, disco_target = features.to(device), labels.to(device), weights.to(device), disco_target.to(device)\n",
    "                    predictions = model(features)\n",
    "                    loss, bce, disco = criterion(predictions, labels, disco_target, weights)\n",
    "                    te_loss += loss.item()\n",
    "                    te_bce += bce.item()\n",
    "                    te_disco += disco.item()\n",
    "    \n",
    "    if epoch > 500 and epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} - Train Loss: {tr_loss/n_batches_train:.3f} - Test Loss: {te_loss/n_batches_test:.3f}\")\n",
    "        torch.save(model.state_dict(), f\"models/model_{epoch}.pt\")\n",
    "\n",
    "    train_loss.append([tr_loss/n_batches_train, tr_bce/n_batches_train, tr_disco/n_batches_train])\n",
    "    test_loss.append([te_loss/n_batches_test, te_bce/n_batches_test, te_disco/n_batches_test])\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model for Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.jit.script(model)\n",
    "sm.save(\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array(train_loss)\n",
    "test_loss = np.array(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax[0].plot(train_loss[:,0], label=\"Total Loss\")\n",
    "ax[0].plot(train_loss[:,1], label=\"BCE Loss\")\n",
    "ax[0].plot(train_loss[:,2], label=\"DisCo Loss\")\n",
    "\n",
    "ax[1].plot(test_loss[:,0], label=\"Total Loss\")\n",
    "ax[1].plot(test_loss[:,1], label=\"BCE Loss\")\n",
    "ax[1].plot(test_loss[:,2], label=\"DisCo Loss\")\n",
    "\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[0].set_title(\"Train Loss\")\n",
    "ax[1].set_title(\"Test Loss\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(test_data.features.to(device))\n",
    "y_score = prediction.squeeze(-1).detach().cpu().numpy()\n",
    "\n",
    "false_positive_rate, true_positive_rate, _ = roc_curve(test_data.labels, y_score)\n",
    "score = auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(false_positive_rate, true_positive_rate, label='ROC curve (area = %0.2f)' % score)\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(\"roc.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
